{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.utils_model import load_model_specific_params\n",
    "from src.utils.prepare import prepare_model\n",
    "\n",
    "def load_model(model_name, checkpoint_path):\n",
    "    model_params = load_model_specific_params(model_name)\n",
    "    model_params = {\n",
    "        'num_classes': 10,\n",
    "        'input_channels': 3,\n",
    "        'img_height': 32,\n",
    "        'img_width': 32,\n",
    "        'overlap': 00,\n",
    "        **model_params\n",
    "        }\n",
    "        \n",
    "    model = prepare_model(model_name, model_params=model_params)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=torch.device('cpu')))\n",
    "    return model\n",
    "\n",
    "\n",
    "def trace_of_total_covariance(neurons):\n",
    "    mean_neuron = neurons.mean(axis=0)\n",
    "    neurons -= mean_neuron\n",
    "    cov_matrix = (neurons.unsqueeze(2) @ neurons.unsqueeze(1)).mean(axis=0)\n",
    "    trace = torch.trace(cov_matrix)\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mm_resnet'\n",
    "checkpoint_path = '/net/pr2/projects/plgrid/plgg_ccbench/bartek/reports2/all_at_once, training with phase1=0, phase2=180, phase3=0, phase4=0, mm_cifar10, mm_resnet, sgd, overlap=0.0_lr=0.5_wd=0.0_lambda=1.0/2023-12-22_03-19-13/checkpoints/model_step_epoch_180.pth'\n",
    "\n",
    "model_clean = load_model(model_name, checkpoint_path)\n",
    "model_clean = model_clean.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_conv_layer = model_clean.conv11.weight.data\n",
    "\n",
    "# Zakładamy, że wagi są w formacie (liczba_filtrów, kanały, wysokość, szerokość)\n",
    "num_filters = first_conv_layer.shape[0]\n",
    "\n",
    "# Ustawienie parametrów wykresu\n",
    "fig, axes = plt.subplots(8, 8, figsize=(12, 12))\n",
    "normalized_weights = (first_conv_layer - first_conv_layer.min()) / (first_conv_layer.max() - first_conv_layer.min())\n",
    "\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Wyświetlenie wag w siatce 8x8\n",
    "    if i < first_conv_layer.shape[0]:  # Sprawdzenie, czy i-ty filtr istnieje\n",
    "        weight_rgb = np.transpose(normalized_weights[i], (1, 2, 0))\n",
    "        ax.imshow(weight_rgb)\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_conv_layer = model_clean.conv21.weight.data\n",
    "\n",
    "# Zakładamy, że wagi są w formacie (liczba_filtrów, kanały, wysokość, szerokość)\n",
    "num_filters = first_conv_layer.shape[0]\n",
    "\n",
    "# Ustawienie parametrów wykresu\n",
    "fig, axes = plt.subplots(8, 8, figsize=(12, 12))\n",
    "normalized_weights = (first_conv_layer - first_conv_layer.min()) / (first_conv_layer.max() - first_conv_layer.min())\n",
    "\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Wyświetlenie wag w siatce 8x8\n",
    "    if i < first_conv_layer.shape[0]:  # Sprawdzenie, czy i-ty filtr istnieje\n",
    "        weight_rgb = np.transpose(normalized_weights[i], (1, 2, 0))\n",
    "        ax.imshow(weight_rgb)\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAPTUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/pytorch/captum\n",
    "%cd captum\n",
    "!git checkout \"optim-wip\"\n",
    "!pip3 install -e .\n",
    "import sys\n",
    "sys.path.append('/content/captum')\n",
    "%cd ..\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import captum\n",
    "import captum.optim as optimviz\n",
    "import torch\n",
    "import torchvision\n",
    "from captum.optim.models import googlenet\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LossFunction = Callable[[Dict[torch.nn.Module, Optional[torch.Tensor]]], torch.Tensor]\n",
    "\n",
    "\n",
    "def show(\n",
    "    x: torch.Tensor, figsize: Optional[Tuple[int, int]] = None, scale: float = 255.0\n",
    ") -> None:\n",
    "    assert x.dim() == 3 or x.dim() == 4\n",
    "    x = x[0] if x.dim() == 4 else x\n",
    "    x = x.cpu().permute(1, 2, 0) * scale\n",
    "    if figsize is not None:\n",
    "        plt.figure(figsize=figsize)\n",
    "    plt.imshow(x.numpy().astype(np.uint8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def vis_neuron_large(\n",
    "    model: torch.nn.Module, target: torch.nn.Module, channel: int, figsize: Tuple[int, int], steps: int\n",
    ") -> None:\n",
    "    image = optimviz.images.NaturalImage(figsize).to(device)\n",
    "    transforms = torch.nn.Sequential(\n",
    "        optimviz.transforms.CenterCrop(figsize),\n",
    "    )\n",
    "    loss_fn = optimviz.loss.NeuronActivation(target, channel)\n",
    "    obj = optimviz.InputOptimization(model=model, loss_function=loss_fn, input_param=image, transform=transforms)\n",
    "    history = obj.optimize(optimviz.optimization.n_steps(steps, False))\n",
    "    return image().detach().numpy()\n",
    "\n",
    "\n",
    "def vis_neuron(\n",
    "    model: torch.nn.Module, target: torch.nn.Module, loss_fn: LossFunction, figsize: Tuple[int, int], steps: int\n",
    ") -> torch.Tensor:\n",
    "    image = optimviz.images.NaturalImage(figsize).to(device)\n",
    "    transforms = torch.nn.Sequential(\n",
    "        # torch.nn.ReflectionPad2d(4),\n",
    "        # optimviz.transform.RandomSpatialJitter(8),\n",
    "        # optimviz.transform.RandomScale(scale=(1, 0.975, 1.025, 0.95, 1.05)),\n",
    "        # torchvision.transforms.RandomRotation(degrees=(-5, 5)),\n",
    "        # optimviz.transform.RandomSpatialJitter(2),\n",
    "        optimviz.transform.CenterCrop(figsize),\n",
    "    )\n",
    "    obj = optimviz.InputOptimization(model, image, transforms, [target], loss_fn)\n",
    "    history = obj.optimize(optimviz.optimization.n_steps(steps, False))\n",
    "    return image()._t\n",
    "\n",
    "\n",
    "def visualize_neuron_list(\n",
    "    model: torch.nn.Module, target: torch.nn.Module, neuron_list: List[int], figsize: Tuple[int, int], steps: int\n",
    ") -> List[torch.Tensor]:\n",
    "    A = []\n",
    "    for n in neuron_list:\n",
    "        loss_fn = optimviz.loss.NeuronActivation(target, n, figsize, steps)\n",
    "        x_out = vis_neuron(model, target, loss_fn)\n",
    "        A.append(x_out.detach())\n",
    "    return A\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "def save_imgs(img1, img2, save_path):\n",
    "    \"\"\"\n",
    "    Funkcja do łączenia dwóch zdjęć w formacie RGB reprezentowanych jako array numpy\n",
    "    i zapisywania ich jako jednego zdjęcia w wskazanym miejscu.\n",
    "\n",
    "    Parametry:\n",
    "    zdjecie1 (numpy array): Pierwsze zdjęcie w formacie RGB.\n",
    "    zdjecie2 (numpy array): Drugie zdjęcie w formacie RGB.\n",
    "    sciezka_zapisu (str): Ścieżka do zapisu połączonego zdjęcia.\n",
    "    \"\"\"\n",
    "    # Sprawdzenie czy oba zdjęcia mają taki sam rozmiar\n",
    "    if img1.shape != img2.shape:\n",
    "        raise ValueError(\"Zdjęcia muszą mieć ten sam rozmiar\")\n",
    "\n",
    "    # Połączenie obu zdjęć w jedno, umieszczając je obok siebie\n",
    "    polaczone_zdjecie = np.concatenate((img1, img2), axis=1)\n",
    "\n",
    "    # Zapisanie połączonych zdjęć do pliku\n",
    "    plt.imsave(save_path, polaczone_zdjecie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = googlenet(pretrained=True).to(device).eval()\n",
    "\n",
    "\n",
    "# Load InceptionV1 model with nonlinear layers replaced by their linear equivalents\n",
    "linear_model = googlenet(pretrained=True, use_linear_modules_only=True).to(device).eval()\n",
    "\n",
    "W_3a_3b = optimviz.circuits.extract_expanded_weights(linear_model, linear_model.mixed3a, linear_model.mixed3b, 5)\n",
    "\n",
    "W_3a_3b_hm = optimviz.weights_to_heatmap_2d(W_3a_3b[379, 147, ...] / W_3a_3b[379, ...].max())\n",
    "show(W_3a_3b_hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (128, 128)\n",
    "steps = 128\n",
    "\n",
    "layer = model_clean.conv11\n",
    "\n",
    "left_branch = Model(model_clean.left_branch)\n",
    "\n",
    "img1 = vis_neuron_large(left_branch, layer, 0, figsize, steps)\n",
    "img2 = vis_neuron_large(left_branch, layer, 1, figsize, steps)\n",
    "# vis_neuron_large(model, model.mixed3b, 379, figsize, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = img1.squeeze().transpose(1,2,0)\n",
    "img2 = img2.squeeze().transpose(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 3)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'feature/mm_resnet/0/0_test.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_imgs(img1, img2, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
     ]
    }
   ],
   "source": [
    "for module in model_clean.right_branch.modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "model_clean.left_branch = Model(model_clean.left_branch)\n",
    "model_clean.right_branch = Model(model_clean.right_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(model_clean.conv11.weight.shape[0]):\n",
    "    print('channel: ', idx)\n",
    "    vis_neuron_large(model_clean.left_branch, model_clean.conv11, idx)\n",
    "    vis_neuron_large(model_clean.right_branch, model_clean.conv21, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clpi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
