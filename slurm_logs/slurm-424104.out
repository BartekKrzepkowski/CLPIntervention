/net/tscratch/people/plgkrzepk/conda_envs/clpi_env/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
wandb: Currently logged in as: bartekk (ideas_cv). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /net/people/plgrid/plgkrzepk/Github/CLPIntervention2/reports/deficit_reverse, sgd, dual_cifar10, mm_resnet_fp_0.0_lr_0.5_wd_0.0001_lr_lambda_0.98 overlap=0.0, phase1/2023-09-21_19-18-12/tensorboard/wandb/run-20230921_191813-upfvmd1t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deficit_reverse, sgd, dual_cifar10, mm_resnet_fp_0.0_lr_0.5_wd_0.0001_lr_lambda_0.98 overlap=0.0, phase1
wandb: â­ï¸ View project at https://wandb.ai/ideas_cv/Critical_Periods_Interventions
wandb: ðŸš€ View run at https://wandb.ai/ideas_cv/Critical_Periods_Interventions/runs/upfvmd1t
0.5 0.0001
Files already downloaded and verified
overlap: 0.0 with_overlap: 0.5
Files already downloaded and verified
overlap: 0.0 with_overlap: 0.5
Files already downloaded and verified
overlap: 0.0 with_overlap: 0.5
liczba parametrÃ³w 11849354
penalized_parameter_names:  ['conv11.weight', 'conv11.bias', 'conv21.weight', 'conv21.bias', 'net1.0.weight', 'net1.0.bias', 'net1.4.0.conv1.weight', 'net1.4.0.conv1.bias', 'net1.4.0.conv2.weight', 'net1.4.0.conv2.bias', 'net1.4.1.conv1.weight', 'net1.4.1.conv1.bias', 'net1.4.1.conv2.weight', 'net1.4.1.conv2.bias', 'net1.5.0.conv1.weight', 'net1.5.0.conv1.bias', 'net1.5.0.conv2.weight', 'net1.5.0.conv2.bias', 'net1.5.0.downsample.0.weight', 'net1.5.0.downsample.0.bias', 'net1.5.1.conv1.weight', 'net1.5.1.conv1.bias', 'net1.5.1.conv2.weight', 'net1.5.1.conv2.bias', 'net2.0.weight', 'net2.0.bias', 'net2.4.0.conv1.weight', 'net2.4.0.conv1.bias', 'net2.4.0.conv2.weight', 'net2.4.0.conv2.bias', 'net2.4.1.conv1.weight', 'net2.4.1.conv1.bias', 'net2.4.1.conv2.weight', 'net2.4.1.conv2.bias', 'net2.5.0.conv1.weight', 'net2.5.0.conv1.bias', 'net2.5.0.conv2.weight', 'net2.5.0.conv2.bias', 'net2.5.0.downsample.0.weight', 'net2.5.0.downsample.0.bias', 'net2.5.1.conv1.weight', 'net2.5.1.conv1.bias', 'net2.5.1.conv2.weight', 'net2.5.1.conv2.bias', 'net3.0.0.conv1.weight', 'net3.0.0.conv1.bias', 'net3.0.0.conv2.weight', 'net3.0.0.conv2.bias', 'net3.0.0.downsample.0.weight', 'net3.0.0.downsample.0.bias', 'net3.0.1.conv1.weight', 'net3.0.1.conv1.bias', 'net3.0.1.conv2.weight', 'net3.0.1.conv2.bias', 'net3.1.0.conv1.weight', 'net3.1.0.conv1.bias', 'net3.1.0.conv2.weight', 'net3.1.0.conv2.bias', 'net3.1.0.downsample.0.weight', 'net3.1.0.downsample.0.bias', 'net3.1.1.conv1.weight', 'net3.1.1.conv1.bias', 'net3.1.1.conv2.weight', 'net3.1.1.conv2.bias', 'fc.weight', 'fc.bias']
Entering deficit phase!!!
Traceback (most recent call last):
  File "/net/tscratch/people/plgkrzepk/conda_envs/clpi_env/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/net/tscratch/people/plgkrzepk/conda_envs/clpi_env/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/net/people/plgrid/plgkrzepk/Github/CLPIntervention2/scripts/python/run_exp_clp_mm_phase1_resnet18.py", line 217, in <module>
    objective('deficit_reverse', EPOCHS, lr, wd)
  File "/net/people/plgrid/plgkrzepk/Github/CLPIntervention2/scripts/python/run_exp_clp_mm_phase1_resnet18.py", line 205, in objective
    trainer.run_exp1_reverse(config)
  File "/net/people/plgrid/plgkrzepk/Github/CLPIntervention2/src/trainer/trainer_classification_dual_clp.py", line 70, in run_exp1_reverse
    self.run_loop(config.epoch_start_at, config.epoch_end_at, config)
  File "/net/people/plgrid/plgkrzepk/Github/CLPIntervention2/src/trainer/trainer_classification_dual_clp.py", line 156, in run_loop
    self.run_epoch(phase='train', config=config)
  File "/net/people/plgrid/plgkrzepk/Github/CLPIntervention2/src/trainer/trainer_classification_dual_clp.py", line 231, in run_epoch
    self.extra_modules['trace_fim'](self.global_step, config, kind=config.kind)
  File "/net/tscratch/people/plgkrzepk/conda_envs/clpi_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/net/people/plgrid/plgkrzepk/Github/CLPIntervention2/src/modules/aux_modules.py", line 150, in forward
    ft_per_sample_grads = self.ft_criterion(params, buffers, config, (x_true1, x_true2))
  File "/net/tscratch/people/plgkrzepk/conda_envs/clpi_env/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 434, in wrapped
    return _flat_vmap(
  File "/net/tscratch/people/plgkrzepk/conda_envs/clpi_env/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 39, in fn
    return f(*args, **kwargs)
  File "/net/tscratch/people/plgkrzepk/conda_envs/clpi_env/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 619, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/net/people/plgrid/plgkrzepk/Github/CLPIntervention2/src/modules/aux_modules.py", line 135, in grad_and_trace
    sample_grads = grad(self.compute_loss, has_aux=False)(params, buffers, config, sample)
  File "/net/tscratch/people/plgkrzepk/conda_envs/clpi_env/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1380, in wrapper
    results = grad_and_value(func, argnums, has_aux=has_aux)(*args, **kwargs)
  File "/net/tscratch/people/plgkrzepk/conda_envs/clpi_env/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 39, in fn
    return f(*args, **kwargs)
  File "/net/tscratch/people/plgkrzepk/conda_envs/clpi_env/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1267, in wrapper
    flat_grad_input = _autograd_grad(flat_outputs, flat_diff_args, create_graph=True)
  File "/net/tscratch/people/plgkrzepk/conda_envs/clpi_env/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 113, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/net/tscratch/people/plgkrzepk/conda_envs/clpi_env/lib/python3.10/site-packages/torch/autograd/__init__.py", line 303, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.39 GiB (GPU 0; 39.42 GiB total capacity; 35.18 GiB already allocated; 2.47 GiB free; 35.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: ðŸš€ View run deficit_reverse, sgd, dual_cifar10, mm_resnet_fp_0.0_lr_0.5_wd_0.0001_lr_lambda_0.98 overlap=0.0, phase1 at: https://wandb.ai/ideas_cv/Critical_Periods_Interventions/runs/upfvmd1t
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./reports/deficit_reverse, sgd, dual_cifar10, mm_resnet_fp_0.0_lr_0.5_wd_0.0001_lr_lambda_0.98 overlap=0.0, phase1/2023-09-21_19-18-12/tensorboard/wandb/run-20230921_191813-upfvmd1t/logs
